fvals[ndim+1] <- f_expand
} else {
simplex[ndim+1,] <- x_reflect
fvals[ndim+1] <- f_reflect
}
} else if (f_reflect < fvals[ndim]) {
# Accept reflection
simplex[ndim+1,] <- x_reflect
fvals[ndim+1] <- f_reflect
} else {
# Contraction
if (f_reflect < fvals[ndim+1]) {
# Outside contraction
x_contract <- centroid + coef_contract * (x_reflect - centroid)
f_contract <- objective(x_contract)
if (f_contract <= f_reflect) {
simplex[ndim+1,] <- x_contract
fvals[ndim+1] <- f_contract
} else {
# Shrink
for (i in 2:(ndim+1)) {
simplex[i,] <- simplex[1,] + coef_shrink * (simplex[i,] - simplex[1,])
fvals[i] <- objective(simplex[i,])
}
}
} else {
# Inside contraction
x_contract <- centroid + coef_contract * (simplex[ndim+1,] - centroid)
f_contract <- objective(x_contract)
if (f_contract < fvals[ndim+1]) {
simplex[ndim+1,] <- x_contract
fvals[ndim+1] <- f_contract
} else {
# Shrink
for (i in 2:(ndim+1)) {
simplex[i,] <- simplex[1,] + coef_shrink * (simplex[i,] - simplex[1,])
fvals[i] <- objective(simplex[i,])
}
}
}
}
}
# Return: minimum value, iterations, function evaluations
return(c(fvals[1], iter, feval_count))
df <- read.table('test.1.tsv', header = TRUE)
result <- neuralNetworkNelderMead(10, df)
print(result)
result <- neuralNetworkNelderMead(10, df)
result <- neuralNetworkNelderMead(10, df)
result <- neuralNetworkNelderMead(10, df)
neuralNetworkNelderMead <- function(p, df) {
X <- df$X; Y <- df$Y; n <- length(Y)
GeLu <- function(x) x * pnorm(x)
f_eval_count <<- 0
F_obj <- function(alpha) {
f_eval_count <<- f_eval_count + 1
alpha0 <- alpha[1]
a1 <- alpha[2:(p + 1)]
a2 <- alpha[(p + 2):(2 * p + 1)]
a3 <- alpha[(2 * p + 2):(3 * p + 1)]
h <- GeLu(outer(X, a3) + matrix(a2, n, p, byrow = TRUE))
pred <- alpha0 + as.vector(h %*% a1)
mean((Y - pred)^2)
}
Nelder.Mead <- function(fn, x0, tol = 1e-5, maxiter = 10000) {
n <- length(x0)
alpha <- 1; gamma <- 2; rho <- 0.5; sigma <- 0.5
simplex <- matrix(0, nrow = n + 1, ncol = n)
simplex[1, ] <- x0
for (i in 1:n) {
y <- x0; y[i] <- if (x0[i] != 0) (1 + 0.05) * x0[i] else 0.00025
simplex[i + 1, ] <- y
}
fvals <- apply(simplex, 1, fn)
iter <- 0
repeat {
iter <- iter + 1
ord <- order(fvals); simplex <- simplex[ord, ]; fvals <- fvals[ord]
best <- simplex[1, ]; worst <- simplex[n + 1, ]
centroid <- colMeans(simplex[1:n, ])
x_reflect <- centroid + alpha * (centroid - worst)
f_reflect <- fn(x_reflect)
if (f_reflect < fvals[1]) {
x_expand <- centroid + gamma * (x_reflect - centroid)
f_expand <- fn(x_expand)
if (f_expand < f_reflect) { simplex[n + 1, ] <- x_expand; fvals[n + 1] <- f_expand }
else { simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect }
} else if (f_reflect < fvals[n]) {
simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect
} else {
x_contract <- if (f_reflect < fvals[n + 1])
centroid + rho * (x_reflect - centroid)
else
centroid - rho * (centroid - worst)
f_contract <- fn(x_contract)
if (f_contract < fvals[n + 1]) {
simplex[n + 1, ] <- x_contract; fvals[n + 1] <- f_contract
} else {
for (i in 2:(n + 1)) {
simplex[i, ] <- simplex[1, ] + sigma * (simplex[i, ] - simplex[1, ])
fvals[i] <- fn(simplex[i, ])
}
}
}
if (sd(fvals) < tol || iter >= maxiter) break
}
c(min(fvals), iter, f_eval_count)
}
x0 <- rep(0, 3 * p + 1)
Nelder.Mead(F_obj, x0)
}
df <- read.table('test.1.tsv', header=TRUE)
result <- neuralNetworkNelderMead(10, df)
print(result)
result <- neuralNetworkNelderMead(10, df)
print(result)
cat("Iteration:", iter, "Best:", f_best, "sd:", sd(fvals), "\n")
cat("Iteration:", iter, "Best:", f_best, "sd:", sd(fvals), "\n")
neuralNetworkNelderMead <- function(p, df) {
X <- df$X; Y <- df$Y; n <- length(Y)
GeLu <- function(x) x * pnorm(x)
f_eval_count <<- 0
F_obj <- function(alpha) {
f_eval_count <<- f_eval_count + 1
alpha0 <- alpha[1]
a1 <- alpha[2:(p + 1)]
a2 <- alpha[(p + 2):(2 * p + 1)]
a3 <- alpha[(2 * p + 2):(3 * p + 1)]
h <- GeLu(outer(X, a3) + matrix(a2, n, p, byrow = TRUE))
pred <- alpha0 + as.vector(h %*% a1)
mean((Y - pred)^2)
}
Nelder.Mead <- function(fn, x0, tol = 1e-5, maxiter = 10000) {
n <- length(x0)
alpha <- 1; gamma <- 2; rho <- 0.5; sigma <- 0.5
simplex <- matrix(0, nrow = n + 1, ncol = n)
simplex[1, ] <- x0
for (i in 1:n) {
y <- x0; y[i] <- if (x0[i] != 0) (1 + 0.05) * x0[i] else 0.00025
simplex[i + 1, ] <- y
}
fvals <- apply(simplex, 1, fn)
iter <- 0
repeat {
iter <- iter + 1
ord <- order(fvals); simplex <- simplex[ord, ]; fvals <- fvals[ord]
best <- simplex[1, ]; worst <- simplex[n + 1, ]
centroid <- colMeans(simplex[1:n, ])
x_reflect <- centroid + alpha * (centroid - worst)
f_reflect <- fn(x_reflect)
if (f_reflect < fvals[1]) {
x_expand <- centroid + gamma * (x_reflect - centroid)
f_expand <- fn(x_expand)
if (f_expand < f_reflect) { simplex[n + 1, ] <- x_expand; fvals[n + 1] <- f_expand }
else { simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect }
} else if (f_reflect < fvals[n]) {
simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect
} else {
x_contract <- if (f_reflect < fvals[n + 1])
centroid + rho * (x_reflect - centroid)
else
centroid - rho * (centroid - worst)
f_contract <- fn(x_contract)
if (f_contract < fvals[n + 1]) {
simplex[n + 1, ] <- x_contract; fvals[n + 1] <- f_contract
} else {
for (i in 2:(n + 1)) {
simplex[i, ] <- simplex[1, ] + sigma * (simplex[i, ] - simplex[1, ])
fvals[i] <- fn(simplex[i, ])
}
}
}
if (sd(fvals) < tol || iter >= maxiter) break
}
c(min(fvals), iter, f_eval_count)
}
x0 <- rep(0, 3 * p + 1)
Nelder.Mead(F_obj, x0)
}
neuralNetworkNelderMead <- function(p, df) {
X <- df$X; Y <- df$Y; n <- length(Y)
GeLu <- function(x) x * pnorm(x)
f_eval_count <<- 0
F_obj <- function(alpha) {
f_eval_count <<- f_eval_count + 1
alpha0 <- alpha[1]
a1 <- alpha[2:(p + 1)]
a2 <- alpha[(p + 2):(2 * p + 1)]
a3 <- alpha[(2 * p + 2):(3 * p + 1)]
h <- GeLu(outer(X, a3) + matrix(a2, n, p, byrow = TRUE))
pred <- alpha0 + as.vector(h %*% a1)
mean((Y - pred)^2)
}
Nelder.Mead <- function(fn, x0, tol = 1e-5, maxiter = 10000) {
n <- length(x0)
alpha <- 1; gamma <- 2; rho <- 0.5; sigma <- 0.5
simplex <- matrix(0, nrow = n + 1, ncol = n)
simplex[1, ] <- x0
for (i in 1:n) {
y <- x0; y[i] <- if (x0[i] != 0) (1 + 0.05) * x0[i] else 0.00025
simplex[i + 1, ] <- y
}
fvals <- apply(simplex, 1, fn)
iter <- 0
repeat {
iter <- iter + 1
ord <- order(fvals); simplex <- simplex[ord, ]; fvals <- fvals[ord]
best <- simplex[1, ]; worst <- simplex[n + 1, ]
centroid <- colMeans(simplex[1:n, ])
x_reflect <- centroid + alpha * (centroid - worst)
f_reflect <- fn(x_reflect)
if (f_reflect < fvals[1]) {
x_expand <- centroid + gamma * (x_reflect - centroid)
f_expand <- fn(x_expand)
if (f_expand < f_reflect) { simplex[n + 1, ] <- x_expand; fvals[n + 1] <- f_expand }
else { simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect }
} else if (f_reflect < fvals[n]) {
simplex[n + 1, ] <- x_reflect; fvals[n + 1] <- f_reflect
} else {
x_contract <- if (f_reflect < fvals[n + 1])
centroid + rho * (x_reflect - centroid)
else
centroid - rho * (centroid - worst)
f_contract <- fn(x_contract)
if (f_contract < fvals[n + 1]) {
simplex[n + 1, ] <- x_contract; fvals[n + 1] <- f_contract
} else {
for (i in 2:(n + 1)) {
simplex[i, ] <- simplex[1, ] + sigma * (simplex[i, ] - simplex[1, ])
fvals[i] <- fn(simplex[i, ])
}
}
}
if (sd(fvals) < tol || iter >= maxiter) break
}
c(min(fvals), iter, f_eval_count)
}
x0 <- rep(0, 3 * p + 1)
Nelder.Mead(F_obj, x0)
}
neuralNetworkNelderMead <- function(p, df) {
# Extract X and Y (both are scalars/vectors)
X <- df$X
Y <- df$Y
n <- length(Y)
# Global counter for function evaluations
feval_count <- 0
# Objective function
objective <- function(params) {
feval_count <<- feval_count + 1
# Extract parameters
a0 <- params[1]
a_out <- params[2:(p+1)]
a_bias <- params[(p+2):(2*p+1)]
a_weight <- params[(2*p+2):(3*p+1)]
# Compute hidden layer: bias + weight * X for each node
# Z is n x p matrix
Z <- outer(X, a_weight) + matrix(a_bias, nrow=n, ncol=p, byrow=TRUE)
# Apply GeLU: GeLU(x) = x * Phi(x)
Z_gelu <- Z * pnorm(Z)
# Compute predictions
predictions <- a0 + Z_gelu %*% a_out
# Mean squared error
return(mean((Y - predictions)^2))
}
# Initialize parameters
ndim <- 3 * p + 1
x0 <- rep(0, ndim)
tol <- 1e-5
max_iter <- 100000
# Initialize simplex with larger step size
simplex <- matrix(0, nrow=ndim+1, ncol=ndim)
simplex[1,] <- x0
for (i in 1:ndim) {
simplex[i+1,] <- x0
if (x0[i] != 0) {
simplex[i+1, i] <- (1 + 0.05) * x0[i]
} else {
simplex[i+1, i] <- 1.0
}
}
# Evaluate initial simplex
fvals <- numeric(ndim+1)
for (i in 1:(ndim+1)) {
fvals[i] <- objective(simplex[i,])
}
# Nelder-Mead coefficients
coef_reflect <- 1.0
coef_expand <- 2.0
coef_contract <- 0.5
coef_shrink <- 0.5
# Main loop
iter <- 0
while (iter < max_iter) {
iter <- iter + 1
# Sort by function value
ord <- order(fvals)
simplex <- simplex[ord, , drop=FALSE]
fvals <- fvals[ord]
# Check convergence
if (max(abs(fvals[1] - fvals[-1])) < tol) {
break
}
# Compute centroid (excluding worst point)
centroid <- colMeans(simplex[1:ndim, , drop=FALSE])
# Reflection
x_reflect <- centroid + coef_reflect * (centroid - simplex[ndim+1,])
f_reflect <- objective(x_reflect)
if (f_reflect < fvals[1]) {
# Try expansion
x_expand <- centroid + coef_expand * (x_reflect - centroid)
f_expand <- objective(x_expand)
if (f_expand < f_reflect) {
simplex[ndim+1,] <- x_expand
fvals[ndim+1] <- f_expand
} else {
simplex[ndim+1,] <- x_reflect
fvals[ndim+1] <- f_reflect
}
} else if (f_reflect < fvals[ndim]) {
# Accept reflection
simplex[ndim+1,] <- x_reflect
fvals[ndim+1] <- f_reflect
} else {
# Contraction
if (f_reflect < fvals[ndim+1]) {
# Outside contraction
x_contract <- centroid + coef_contract * (x_reflect - centroid)
f_contract <- objective(x_contract)
if (f_contract <= f_reflect) {
simplex[ndim+1,] <- x_contract
fvals[ndim+1] <- f_contract
} else {
# Shrink
for (i in 2:(ndim+1)) {
simplex[i,] <- simplex[1,] + coef_shrink * (simplex[i,] - simplex[1,])
fvals[i] <- objective(simplex[i,])
}
}
} else {
# Inside contraction
x_contract <- centroid + coef_contract * (simplex[ndim+1,] - centroid)
f_contract <- objective(x_contract)
if (f_contract < fvals[ndim+1]) {
simplex[ndim+1,] <- x_contract
fvals[ndim+1] <- f_contract
} else {
# Shrink
for (i in 2:(ndim+1)) {
simplex[i,] <- simplex[1,] + coef_shrink * (simplex[i,] - simplex[1,])
fvals[i] <- objective(simplex[i,])
}
}
}
}
}
# Return: minimum value, iterations, function evaluations
return(c(fvals[1], iter, feval_count))
}
usethis::create_package("scadlla")
setwd("/Users/mengfanlong/Downloads/Projects/Biostats 615/scadLLA")
Rcpp::compileAttributes()
devtools::install()
# 设置工作目录
setwd("/Users/mengfanlong/Downloads/Projects/Biostats 615/scadLLA")
# 运行模拟数据示例（自动使用 Rcpp 加速）
source("examples/dynamic_scad_simulated_data.r")
# 或运行真实数据示例
source("examples/dynamic_scad_real_data.r")
# 设置工作目录
setwd("/Users/mengfanlong/Downloads/Projects/Biostats 615/scadLLA")
# 运行模拟数据示例（自动使用 Rcpp 加速）
source("examples/dynamic_scad_simulated_data.r")
# 或运行真实数据示例
source("examples/dynamic_scad_real_data.r")
# 设置工作目录
setwd("/Users/mengfanlong/Downloads/Projects/Biostats 615/scadLLA")
# 运行模拟数据示例（现在应该可以工作了）
source("examples/dynamic_scad_simulated_data.r")
# 运行真实数据示例（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行真实数据对比
source("examples/static_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行真实数据对比
source("examples/static_scad_real_data.r")
source("examples/static_scad_real_data.r")
source("examples/static_scad_real_data.r")
source("examples/static_scad_real_data.r")
source("examples/static_scad_real_data.r")
source("examples/static_scad_real_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行真实数据对比
source("examples/static_scad_real_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行真实数据对比
source("examples/static_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
#
source("compile_rcpp.R")
#
source("R/compile_rcpp.R")
library(scadLLA)
exists("solve_dynamic_lqa_admm_cpp", mode = "function")  # 应该返回 TRUE
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
Rscript -e "Rcpp::compileAttributes()"
R CMD build .
R CMD INSTALL scadLLA_0.1.0.tar.gz
Rscript -e "Rcpp::compileAttributes()"
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行改进后的模拟（应该看到更好的结果）
source("examples/dynamic_scad_simulated_data.r")
# 运行修复后的真实数据（MV 收益现在应该不同了）
source("examples/dynamic_scad_real_data.r")
# 运行模拟数据对比
source("examples/static_scad_simulated_data.r")
# 运行真实数据对比
source("examples/static_scad_real_data.r")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
library(Scad)
install.packages("Scad")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
R CMD build .
install.packages("Scad_0.1.0.tar.gz", repos = NULL, type = "source")
library(Scad)
ls("package:Scad")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
remove.packages("Scad")
remove.packages("scadLLA")
# install.packages("devtools")
devtools::install_github("kevinlmf/Scad")
remove.packages("scadLLA")
library(Scad)
# 使用静态 SCAD
lqa_scad(X, y, lambda = 0.1)
